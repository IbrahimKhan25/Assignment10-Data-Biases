{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c65c8e7",
   "metadata": {},
   "source": [
    "#  Forming Hypothesis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "08a51494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most common toxic words:\n",
      "nigger: 1467\n",
      "faggot: 1049\n",
      "boob: 1001\n",
      "poop: 987\n",
      "hate: 955\n",
      "gay: 941\n",
      "youi: 886\n",
      "niggers: 884\n",
      "penis: 624\n",
      "stupid: 618\n"
     ]
    }
   ],
   "source": [
    "import csv \n",
    "from collections import Counter # using counter to avoid a large helper function\n",
    "\n",
    "toxic_words = [] # will store words from sentences labeled toxic\n",
    "\n",
    "with open('Sample_labaled_data.csv', newline = '') as csvfile:\n",
    "    \n",
    "    reader = csv.reader(csvfile)\n",
    "    \n",
    "    all_words = [] # all words\n",
    "\n",
    "    for row in reader:\n",
    "        words = row[2].lower().split() # splits sentences up into list of words\n",
    "        all_words.extend(words)        # updates all_words with the words in each sentence \n",
    "        \n",
    "        if row[3] == 'yes': # if sentence is labeled toxic, add the toxic_words\n",
    "            toxic_words.extend(words)\n",
    "\n",
    "all_word_count = Counter(all_words) # makes a count of all words\n",
    "most_common_all = all_word_count.most_common(50) # get the top 50 most common words\n",
    "\n",
    "exclude_words = []\n",
    "for i in range(len(most_common_all)): # adds top 50 most common words to exclude_words\n",
    "    exclude_words.append(most_common_all[i][0])\n",
    "\n",
    "# remove most common words from toxic_words to avoid over inflated results \n",
    "toxic_words = [word for word in toxic_words if word not in exclude_words]\n",
    "   \n",
    "toxic_word_count = Counter(toxic_words) # counts all toxic words\n",
    "most_common_toxic_words = toxic_word_count.most_common(10) # gets top 10 most toxic words \n",
    "    \n",
    "print(\"Top 10 most common toxic words:\")\n",
    "for word, count in most_common_toxic_words:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bcb234",
   "metadata": {},
   "source": [
    "### my hypothesis is that the API disregards context. It looks as if the API assigns toxicity values to words, and calculates the total toxicity of a phrase using the individual toxicity of each word. This means possibly non toxic phrases with toxic words, meaninglessly placed with 0 context, may still show up as toxic. and similarly toxic phrases with very non-toxic words may show up as non-toxic\n",
    "\n",
    "### that may be why the most common toxic words are accurate, since if one of these words is included in a phrase it is likely to be considered toxic\n",
    "\n",
    "### but that also means if one of these words is in a phrase that is not considered toxic, the API may still flag it as being toxic\n",
    "### \n",
    "\n",
    "### for the sake of this assignment lets assume a probability of over 0.5 is considered toxic\n",
    "### i will now test my hypothesis by choosing phrases from the sample csv that included the most common toxic words, but were labeled as 'no' for toxicitiy. some of the sentenced have been edited to further prove my hypothesis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a25d34",
   "metadata": {},
   "source": [
    "# DISCLAIMER:  to test my hypothesis I will be using the most common toxic words found above, this is purely to test the functionality of the API and is by no means meant to offend or disrespect anyone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c5cb8106",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    # test 1\n",
    "    (# found in sample csv\n",
    "    \"This message is regarding the page Nigger Thanks for experimenting with Wikipedia  Your test worked and has been removed Please use the sandbox for any other tests you want to do Take a look at the welcome page if you would like to learn more about contributing  Thanks\", \n",
    "    \n",
    "    # randomly altered with toxic words\n",
    "    \"This message is regarding the page Nigger Thanks for experimenting with Wikipedia  Your test worked faggot and has been removed Please use the sandbox for any other tests you want to do Take a look at the boob welcome page if you would like to learn more about contributing  Thanks\"\n",
    "    ),\n",
    "    \n",
    "    # test 2\n",
    "    (# found in sample csv\n",
    "    \"black people deserve to die\", \n",
    "    # randomly altered with positive words\n",
    "    \"black people beautiful deserve peace to die\"\n",
    "    ), \n",
    "    \n",
    "    # test 3\n",
    "    (# found in sample csv\n",
    "    \"I knew he was a dirty jew\", \n",
    "    # randomly altered with positive words\n",
    "    \"I knew he good was a dirty jew happiness\"\n",
    "    ), \n",
    "    \n",
    "    # test 4\n",
    "    (# found in sample csv\n",
    "    \"black people deserve to die\", \n",
    "    # intentionally changed to test context\n",
    "    \"black people dont deserve to die\"\n",
    "    ), \n",
    "    \n",
    "    (# found in sample csv\n",
    "    \"I knew he was a dirty jew\", \n",
    "    # intentionally changed to test context\n",
    "    \"I knew he was a dirty jew, not\"\n",
    "    ), \n",
    "    # test 6\n",
    "    (# intentionally made to test hypothesis \n",
    "    \"i care that he was a person of color\", \n",
    "    # intentionally made to test hypothesis\n",
    "    \"i dont care that he was a person of color\"\n",
    "    ), \n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "706d8157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1\n",
      "Toxicity of sample csv:0.40891263\n",
      "Toxicity of altered phrase:0.6342749\n",
      "\n",
      "Test 2\n",
      "Toxicity of sample csv:0.9288007\n",
      "Toxicity of altered phrase:0.687436\n",
      "\n",
      "Test 3\n",
      "Toxicity of sample csv:0.7856813\n",
      "Toxicity of altered phrase:0.6020386\n",
      "\n",
      "Test 4\n",
      "Toxicity of sample csv:0.9288007\n",
      "Toxicity of altered phrase:0.30439767\n",
      "\n",
      "Test 5\n",
      "Toxicity of sample csv:0.7856813\n",
      "Toxicity of altered phrase:0.6491204\n",
      "\n",
      "Test 6\n",
      "Toxicity of sample csv:0.30136615\n",
      "Toxicity of altered phrase:0.4269174\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_count = 0\n",
    "\n",
    "for test in test_cases:\n",
    "    analyze_request = {\n",
    "  'comment': { 'text': test[0] },\n",
    "  'requestedAttributes': {'TOXICITY': {}}\n",
    "}\n",
    "    response = client.comments().analyze(body=analyze_request).execute()\n",
    "    toxicity0 = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "    \n",
    "    analyze_request = {\n",
    "  'comment': { 'text': test[1] },\n",
    "  'requestedAttributes': {'TOXICITY': {}}\n",
    "}\n",
    "    response = client.comments().analyze(body=analyze_request).execute()\n",
    "    toxicity1 = response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "    \n",
    "    test_count += 1\n",
    "    print(\"Test \" + str(test_count))\n",
    "    print(\"Toxicity of sample csv:\" + str(toxicity0))\n",
    "    print(\"Toxicity of altered phrase:\" + str(toxicity1)+ \"\\n\")\n",
    "     \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deed6e1",
   "metadata": {},
   "source": [
    "## It seems my hypothesis was somewhat wrong\n",
    "## although it does look like randomly adding toxic words without context (test 1) does increase toxicity probability, the API does indeed take context into account(test 4, 5), \n",
    "## a few interesting things i noticed was that randomly adding positive words (test 2, 3) had little effect on the toxic probability. Also, test 6 contradicts my conclusion that the API does take context into account, as it gave a lower score for a phrase that may be considered toxic\n",
    "## although my sample size is low it allows me to pinpoint the problems with my hypothesis and obtain conclusive results\n",
    "## In conclusion, my hypothesis that the API does not take context into account, and just assigns words a toxic value was partially wrong, as I was able to conclude that the API does indeed contexualize phrases, but it is also affected by random insertions of words, with toxic words having far more noticable effects than positive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27dccd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
